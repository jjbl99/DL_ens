{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-belief",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os \n",
    "import sklearn\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/julesbaudet/Documents/0. Cours/ENS/Deep Learning DIY/Projet final/ens_data\"\n",
    "# path = \"/Users/linusbleistein/Documents/Cours ENS/Cours mathÃ©matiques/Deep learning 2020-2021/data_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path,\"clean_brave_data.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-street",
   "metadata": {},
   "source": [
    "## Creating a data  embedding\n",
    "\n",
    "To be able to learn a classification of nodes on our graph, we need to transform the data on nodes to be able to express some of the information contained in graph form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of adjacency with line data relative to sent transactions only \n",
    "# %%time\n",
    "# from_c = CategoricalDtype(sorted(df.from_address.unique()), ordered=True)\n",
    "# to_c = CategoricalDtype(sorted(df.to_address.unique()), ordered=True)\n",
    "# \n",
    "# row = df.from_address.astype(from_c).cat.codes\n",
    "# col = df.to_address.astype(to_c).cat.codes\n",
    "# sparse = csr_matrix((df[\"value\"], (row, col)), \\\n",
    "#                            shape=(from_c.categories.size, to_c.categories.size))\n",
    "# print(sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of adjacency with line data relative to sent and received transactions \n",
    "%%time\n",
    "# first we add addresses that only received but not sent to first column \n",
    "to_add = df.loc[~df['to_address'].isin(df['from_address'])].copy()\n",
    "to_add = to_add.rename(columns={'to_address':'from_address','from_address':'to_address'})\n",
    "to_add['value'] = 0\n",
    "\n",
    "df = pd.concat((df,to_add))\n",
    "\n",
    "categories = CategoricalDtype(sorted(df.from_address.unique()), ordered=True) # will have all nodes! use it for all \n",
    "row = df.to_address.astype(categories).cat.codes\n",
    "col = df.from_address.astype(categories).cat.codes\n",
    "\n",
    "# sparse matrix for values sent \n",
    "sp = csr_matrix((df['value'],(row, col)), shape=(categories.categories.size, categories.categories.size)\n",
    "                \n",
    "# sparse matrix for values received \n",
    "sp2 = csr_matrix((-df['value'],(col, row)), shape=(categories.categories.size, categories.categories.size))\n",
    "                \n",
    "# our data \n",
    "X = scipy.sparse.hstack((sp,sp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-developer",
   "metadata": {},
   "source": [
    "##  Dimension Reduction   \n",
    "\n",
    "Our data has particularly high dimension. We use truncated SVD to reduce our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsvd = TruncatedSVD(n_components=65, random_state=42)\n",
    "out = tsvd.fit_transform(X)\n",
    "print(f'our data has shape {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
    "print(tsvd_var_ratios.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken on https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    total_variance = 0.0\n",
    "    n_components = 0\n",
    "    for explained_variance in var_ratio:\n",
    "        total_variance += explained_variance\n",
    "        n_components += 1\n",
    "        if total_variance >= goal_var:\n",
    "            break\n",
    "    return n_components\n",
    "\n",
    "var_goal =  0.99\n",
    "print(f'to keep {var_goal*100}% of variance, one should keep {select_n_components(tsvd_var_ratios,var_goal)} components')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-mother",
   "metadata": {},
   "source": [
    "##  Adding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the known exchange list \n",
    "exch_labels = pd.read_csv(os.path.join(path, \"exchanges_encoded.csv\"), \n",
    "                          delimiter=';', names  = ['address','label']).set_index('address')\n",
    "\n",
    "labels = exch_labels.to_dict()['label']\n",
    "\n",
    "labels2int = dict(zip(labels.values(),[i for i in range(len(labels))])) # keys = addresses, values = name of the exchange\n",
    "address2int = {k:labels2int[labels[k]] for k in labels.keys()}\n",
    "\n",
    "known_addresses = list(labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.vectorize(address2int.get)(np.array(categories.categories))\n",
    "labels = np.nan_to_num(labels.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(out)\n",
    "# data['label'] = np.isin(np.array(sorted(df.from_address.unique())),known_addresses).astype(int)\n",
    "data['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(path,\"processed_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-special",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-archives",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debugging example\n",
    "# a = pd.DataFrame({\"col\": [15, 32, 3, 8], \"col2\":[26,3,17,20], \"value\":[1,1,1,1]})\n",
    "# \n",
    "# # add this to the sparse matrix \n",
    "# add = a.loc[~a['col2'].isin(a['col'])].copy()\n",
    "# add = add.rename(columns={'col':'col2','col2':'col'})\n",
    "# #add['value'] = 0 # -to_add['value']\n",
    "# \n",
    "# a = pd.concat((a,add))\n",
    "# \n",
    "# categories = CategoricalDtype(sorted(a.col.unique()), ordered=True) # will have all nodes! use it for all \n",
    "# row = a.col.astype(categories).cat.codes\n",
    "# col = a.col2.astype(categories).cat.codes\n",
    "# sp = csr_matrix((a['value'],(row, col)), shape=(categories.categories.size, categories.categories.size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
